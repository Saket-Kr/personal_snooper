# SQL Logging Implementation

## Overview

The desktop activity tracker now includes local SQLite database logging alongside the existing Kafka streaming functionality. This provides a local backup of all events and enables offline querying and analysis.

## Features

### Event Storage
- All activity events are automatically logged to a local SQLite database
- Events are stored with full metadata including timestamps, user IDs, and event-specific data
- Database is stored in the user's application data directory for privacy

### Database Schema

The `events` table contains the following columns:

| Column | Type | Description |
|--------|------|-------------|
| `id` | INTEGER | Primary key, auto-increment |
| `event_id` | TEXT | Unique event identifier (UUID) |
| `timestamp` | TEXT | ISO 8601 timestamp |
| `user_id` | TEXT | User identifier |
| `event_type` | TEXT | Event type (APP_ACTIVE, BROWSER_TAB_ACTIVE, FILE_CHANGED) |
| `app_name` | TEXT | Application name (for app events) |
| `process_id` | INTEGER | Process ID (for app events) |
| `app_path` | TEXT | Application path (for app events) |
| `window_title` | TEXT | Window title (for app events) |
| `tab_title` | TEXT | Tab title (for browser events) |
| `tab_url` | TEXT | Tab URL (for browser events) |
| `domain` | TEXT | Domain (for browser events) |
| `file_path` | TEXT | File path (for file events) |
| `file_name` | TEXT | File name (for file events) |
| `file_extension` | TEXT | File extension (for file events) |
| `directory` | TEXT | Directory path (for file events) |
| `change_type` | TEXT | Change type (CREATED, MODIFIED, DELETED) |
| `file_size` | INTEGER | File size in bytes (for file events) |
| `created_at` | DATETIME | Database insertion timestamp |

### Performance Optimizations

- **Indexing**: Database includes indexes on `timestamp`, `user_id`, and `event_type` for fast queries
- **Batch Insertion**: Events are buffered and inserted in batches for better performance
- **Transaction Support**: Batch insertions use transactions for data integrity
- **Automatic Cleanup**: Configurable cleanup of old events to manage database size

## Usage

### Automatic Logging

Events are automatically logged when the agent is running. No additional configuration is required.

### Querying Events

Use the `DatabaseUtils` class to query events:

```typescript
import { DatabaseUtils } from './src/agent/services/db-utils';

const dbUtils = new DatabaseUtils();

// Get total event count
const count = await dbUtils.getEventCount();

// Get recent events
const recentEvents = await dbUtils.getRecentEvents(50);

// Get events by time range
const events = await dbUtils.getEventsByTimeRange(
    '2024-01-01T00:00:00Z',
    '2024-01-02T00:00:00Z'
);

// Get events by type
const appEvents = await dbUtils.getEventsByType('APP_ACTIVE');

// Get database statistics
const stats = await dbUtils.getDatabaseStats();
```

### Testing

Run the SQL logging test:

```bash
npm run test:sql
```

This will display database statistics and recent events.

## Database Location

The SQLite database is stored in the user's application data directory:

- **macOS**: `~/Library/Application Support/desktop-activity-tracker/events.db`
- **Windows**: `%APPDATA%/desktop-activity-tracker/events.db`
- **Linux**: `~/.config/desktop-activity-tracker/events.db`

## Configuration

### Cleanup Settings

You can configure automatic cleanup of old events:

```typescript
// Clean up events older than 30 days
const deletedCount = await dbUtils.cleanupOldEvents(30);
```

### Buffer Settings

The SQL logger uses buffering for performance:

- **Buffer Size**: Events are flushed when buffer reaches 50 events
- **Flush Interval**: Buffer is flushed every 10 seconds
- **Transaction Size**: Up to 50 events per transaction

## Integration

The SQL logging is integrated into the existing event flow:

1. Events are generated by monitors (AppMonitor, FileSystemMonitor)
2. Events are published to the event bus
3. Both KafkaEmitter and SQLLogger subscribe to the event bus
4. Events are sent to Kafka AND logged to SQLite database

This ensures that events are both streamed to Kafka and stored locally for backup and analysis.

## Error Handling

- **Connection Errors**: Events are buffered when database is unavailable
- **Insertion Errors**: Failed insertions are retried on next flush
- **Transaction Rollback**: Failed transactions are rolled back and events re-buffered
- **Graceful Shutdown**: Remaining events are flushed before shutdown

## Monitoring

The SQL logger provides console output for monitoring:

- Connection status messages
- Batch insertion counts
- Error messages for debugging

Example output:
```
[SQLLogger] Connected to SQLite database
[SQLLogger] Logged 25 events
[SQLLogger] Disconnected from SQLite database
```
